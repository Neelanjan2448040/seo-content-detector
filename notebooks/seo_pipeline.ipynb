{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffaf7388-4fda-4c58-86ea-f3bf586848a4",
   "metadata": {},
   "source": [
    "## Importing packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c40d76d-68a8-45d3-8e4b-650a79bdd75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import pkgutil\n",
    "\n",
    "required = [\n",
    "    \"beautifulsoup4\",\n",
    "    \"lxml\",\n",
    "    \"requests\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"nltk\",\n",
    "    \"textstat\",\n",
    "    \"sentence_transformers\",\n",
    "    \"tqdm\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423bee08-7ef0-4cac-83fa-4a3cc3aa612d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Neelanjan Dutta\\AppData\\Local\\Temp\\ipykernel_3104\\2405830571.py:2: DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n",
      "  if not pkgutil.find_loader(pkg):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing beautifulsoup4...\n",
      "Installing scikit-learn...\n"
     ]
    }
   ],
   "source": [
    "for pkg in required:\n",
    "    if not pkgutil.find_loader(pkg):\n",
    "        try:\n",
    "            print(f\"Installing {pkg}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "        except Exception as e:\n",
    "            print(f\"Could not install {pkg}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a292988-ab72-4cf0-9769-2936228748b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Neelanjan\n",
      "[nltk_data]     Dutta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_transformers not available — will fallback to TF-IDF+SVD for embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
    "\n",
    "import textstat\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    have_sbert = True\n",
    "except Exception:\n",
    "    have_sbert = False\n",
    "    print(\"sentence_transformers not available — will fallback to TF-IDF+SVD for embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "924511ef-e964-4a8a-8056-5edbbe8e2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CSV_PATH = \"G:/seo-content-detector/data/data.csv\"\n",
    "EXTRACTED_CSV = \"G:/seo-content-detector/data/extracted_pages.csv\"\n",
    "FEATURES_CSV = \"G:/seo-content-detector/data/features.csv\"\n",
    "DUPLICATES_CSV = \"G:/seo-content-detector/data/duplicates.csv\"\n",
    "QUALITY_CSV = \"G:/seo-content-detector/data/quality_results.csv\"\n",
    "\n",
    "USER_AGENT = 'Mozilla/5.0 (compatible; SEOQualityBot/1.0; +https://example.com/bot)'\n",
    "REQUEST_DELAY = 1.2  # seconds between requests when scraping\n",
    "SIMILARITY_THRESHOLD = 0.80\n",
    "EMBEDDING_DIM = 384 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ac93b61-5b93-4caf-b668-9b1e709f4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({'User-Agent': USER_AGENT})\n",
    "\n",
    "\n",
    "def safe_get(url, timeout=10):\n",
    "    try:\n",
    "        r = session.get(url, timeout=timeout)\n",
    "        r.raise_for_status()\n",
    "        return r.text\n",
    "    except Exception as e:\n",
    "        print(f\"Request failed for {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f99d05e-8871-4f36-84aa-9fae9c5ab42a",
   "metadata": {},
   "source": [
    "## HTML Parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a15de7a4-739c-437d-8258-4ad1bcf59bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_html(html, url=None):\n",
    "    \"\"\"Extract title and main body text from HTML content.\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "    except Exception:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Title\n",
    "    title = ''\n",
    "    if soup.title and soup.title.string:\n",
    "        title = soup.title.string.strip()\n",
    "\n",
    "    # Try common containers for main content\n",
    "    candidates = []\n",
    "    for selector in ['article', 'main', 'div[class*=\"content\"]', 'div[class*=\"post\"]',\n",
    "                     'div[id*=\"content\"]', 'section']:\n",
    "        found = soup.select(selector)\n",
    "        if found:\n",
    "            for f in found:\n",
    "                text = f.get_text(separator=' ', strip=True)\n",
    "                if len(text.split()) > 30:\n",
    "                    candidates.append(text)\n",
    "    # fallback to concatenation of <p> tags\n",
    "    if not candidates:\n",
    "        p_tags = soup.find_all('p')\n",
    "        p_texts = [p.get_text(separator=' ', strip=True) for p in p_tags if p.get_text(strip=True)]\n",
    "        # join paragraphs\n",
    "        joined = '\\n\\n'.join(p_texts)\n",
    "        candidates.append(joined)\n",
    "\n",
    "    # choose the longest candidate\n",
    "    body_text = max(candidates, key=lambda t: len(t)) if candidates else ''\n",
    "\n",
    "    # Clean excessive whitespace\n",
    "    body_text = ' '.join(body_text.split())\n",
    "\n",
    "    # Word count\n",
    "    words = body_text.split()\n",
    "    word_count = len(words)\n",
    "\n",
    "    return {\n",
    "        'title': title,\n",
    "        'body_text': body_text,\n",
    "        'word_count': word_count\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5d05fc-e737-4f81-97cd-83b6e7650653",
   "metadata": {},
   "source": [
    "## Reading input csv and extracting content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91a95c5-d55c-4c8c-ac57-36732268676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_extracted_csv(input_csv=INPUT_CSV_PATH, output_csv=EXTRACTED_CSV, force_scrape=False):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    # Expect columns: url, html_content (optional)\n",
    "\n",
    "    results = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc='Processing rows'):\n",
    "        url = row.get('url') if 'url' in row.index else None\n",
    "        html = None\n",
    "        if 'html_content' in row.index and not pd.isna(row['html_content']) and not force_scrape:\n",
    "            html = row['html_content']\n",
    "        else:\n",
    "            if not url:\n",
    "                print(f\"Row {idx} has no url and no html_content. Skipping\")\n",
    "                continue\n",
    "            html = safe_get(url)\n",
    "            time.sleep(REQUEST_DELAY)\n",
    "\n",
    "        if not html:\n",
    "            # write empty placeholders\n",
    "            results.append({'url': url, 'title': '', 'body_text': '', 'word_count': 0})\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            extracted = extract_from_html(html, url=url)\n",
    "            results.append({'url': url, 'title': extracted['title'], 'body_text': extracted['body_text'], 'word_count': extracted['word_count']})\n",
    "        except Exception as e:\n",
    "            print(f\"Extraction failed for {url}: {e}\")\n",
    "            results.append({'url': url, 'title': '', 'body_text': '', 'word_count': 0})\n",
    "\n",
    "    outdf = pd.DataFrame(results)\n",
    "    outdf.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved extracted content to {output_csv}\")\n",
    "    return outdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53310316-b749-4a6a-8ffb-42014ab085fa",
   "metadata": {},
   "source": [
    "## Feature engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93538d77-8f00-4ba6-bee1-adddaa43f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def compute_readability(text):\n",
    "    try:\n",
    "        # textstat returns a float. If failure, return np.nan\n",
    "        return float(textstat.flesch_reading_ease(text))\n",
    "    except Exception:\n",
    "        # fallback: compute approximate Flesch score\n",
    "        try:\n",
    "            sentences = max(1, len(sent_tokenize(text)))\n",
    "            words = word_tokenize(text)\n",
    "            word_count = len(words) if words else 0\n",
    "            # estimate syllables roughly using textstat if available else naive rule\n",
    "            syllables = textstat.syllable_count(text) if hasattr(textstat, 'syllable_count') else max(1, int(0.5 * word_count))\n",
    "            flesch = 206.835 - 1.015 * (word_count / sentences) - 84.6 * (syllables / max(1, word_count))\n",
    "            return float(flesch)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "\n",
    "def top_keywords_tfidf(texts, top_n=5, ngram_range=(1,2), max_features=5000):\n",
    "    # texts: list of documents. We'll compute TF-IDF and return top keywords per document\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features, ngram_range=ngram_range)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "    top_keywords = []\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X[i].toarray().ravel()\n",
    "        if row.sum() == 0:\n",
    "            top_keywords.append([])\n",
    "            continue\n",
    "        topn_ids = row.argsort()[-top_n:][::-1]\n",
    "        top_keywords.append(feature_names[topn_ids].tolist())\n",
    "    return top_keywords, vectorizer, X\n",
    "\n",
    "\n",
    "# Embeddings: try SBERT, else TF-IDF + TruncatedSVD\n",
    "\n",
    "def get_embeddings(documents, model_name='all-MiniLM-L6-v2', svd_dim=128):\n",
    "    documents = [d if isinstance(d, str) else '' for d in documents]\n",
    "    if have_sbert:\n",
    "        try:\n",
    "            model = SentenceTransformer(model_name)\n",
    "            embs = model.encode(documents, show_progress_bar=True, batch_size=32)\n",
    "            return np.array(embs)\n",
    "        except Exception as e:\n",
    "            print(f\"SBERT encoding failed: {e} — falling back to TF-IDF+SVD\")\n",
    "\n",
    "    # fallback\n",
    "    tfidf = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X = tfidf.fit_transform(documents)\n",
    "    # reduce to dense vector via SVD\n",
    "    svd = TruncatedSVD(n_components=min(svd_dim, X.shape[1]-1 if X.shape[1]>1 else 1), random_state=42)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "    # normalize rows\n",
    "    norms = np.linalg.norm(X_reduced, axis=1, keepdims=True)\n",
    "    norms[norms==0] = 1.0\n",
    "    X_reduced = X_reduced / norms\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b52f4b0-8a26-4fbd-95f1-6044d8403f8a",
   "metadata": {},
   "source": [
    "## Build features data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1330fa2d-4ff6-4cf3-89da-801cc34fd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(extracted_df, features_csv=FEATURES_CSV):\n",
    "    df = extracted_df.copy()\n",
    "    df['text_clean'] = df['body_text'].fillna('').str.lower().str.replace('\\\\s+', ' ', regex=True).str.strip()\n",
    "    df['sentence_count'] = df['text_clean'].apply(lambda t: len(sent_tokenize(t)) if t.strip() else 0)\n",
    "    df['flesch_reading_ease'] = df['text_clean'].apply(lambda t: compute_readability(t) if t.strip() else np.nan)\n",
    "\n",
    "    texts = df['text_clean'].tolist()\n",
    "    top_keywords, tfidf_vectorizer, tfidf_matrix = top_keywords_tfidf(texts, top_n=5)\n",
    "    df['top_keywords'] = top_keywords\n",
    "\n",
    "    # === NEW EMBEDDING LOGIC ===\n",
    "    # We fit the models here, assuming SBERT fallback\n",
    "    print(\"Fitting TF-IDF and SVD models...\")\n",
    "    tfidf_model = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "    X_tfidf = tfidf_model.fit_transform(texts)\n",
    "\n",
    "    # Determine SVD components, using 128 as the target (from old get_embeddings)\n",
    "    n_comps = min(128, X_tfidf.shape[1] - 1 if X_tfidf.shape[1] > 1 else 1)\n",
    "    if n_comps <= 0: n_comps = 1 # Failsafe for empty/tiny corpus\n",
    "\n",
    "    svd_model = TruncatedSVD(n_components=n_comps, random_state=42)\n",
    "    embeddings = svd_model.fit_transform(X_tfidf)\n",
    "\n",
    "    # Normalize\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    norms[norms==0] = 1.0\n",
    "    embeddings = embeddings / norms\n",
    "    \n",
    "    print(f\"Created embeddings with shape {embeddings.shape}\")\n",
    "    \n",
    "    df['embedding'] = embeddings.tolist()\n",
    "    df['is_thin'] = df['word_count'] < 500\n",
    "    df.to_csv(features_csv, index=False)\n",
    "    print(f\"Saved features to {features_csv}\")\n",
    "    \n",
    "    # === RETURN THE FITTED MODELS ===\n",
    "    return df, embeddings, tfidf_model, svd_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c69f08-a972-4b39-897c-2dbd307b8e1e",
   "metadata": {},
   "source": [
    "## Duplicate detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e562ad3-be37-4d59-9030-e3352d0d5098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_duplicates(df, embeddings, threshold=SIMILARITY_THRESHOLD, output_csv=DUPLICATES_CSV):\n",
    "    # Compute pairwise cosine similarity\n",
    "    if embeddings is None or len(embeddings) == 0:\n",
    "        print(\"No embeddings provided\")\n",
    "        return pd.DataFrame(columns=['url1','url2','similarity'])\n",
    "\n",
    "    # If embeddings are not normalized, cosine_similarity still works\n",
    "    sim_matrix = cosine_similarity(embeddings)\n",
    "    n = sim_matrix.shape[0]\n",
    "    pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            sim = float(sim_matrix[i,j])\n",
    "            if sim >= threshold:\n",
    "                pairs.append({'url1': df.iloc[i]['url'], 'url2': df.iloc[j]['url'], 'similarity': sim})\n",
    "    pairs_df = pd.DataFrame(pairs)\n",
    "    pairs_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(pairs)} duplicate pairs to {output_csv}\")\n",
    "    return pairs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae64c9b-3038-44b0-b3bb-3b9d57cf02fc",
   "metadata": {},
   "source": [
    "## Labelling and classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1dc5062-2c6e-44dd-8139-4f46e468acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quality_labels(df):\n",
    "    # Synthetic labels per assignment rules\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        wc = row['word_count'] if not pd.isna(row['word_count']) else 0\n",
    "        fr = row['flesch_reading_ease'] if not pd.isna(row['flesch_reading_ease']) else -999\n",
    "        if wc > 1500 and 50 <= fr <= 70:\n",
    "            labels.append('High')\n",
    "        elif wc < 500 or fr < 30:\n",
    "            labels.append('Low')\n",
    "        else:\n",
    "            labels.append('Medium')\n",
    "    df['quality_label'] = labels\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_quality_model(df, features_to_use=['word_count','sentence_count','flesch_reading_ease']):\n",
    "    # Prepare dataset\n",
    "    X = df[features_to_use].fillna(0).values\n",
    "    y = df['quality_label'].values\n",
    "\n",
    "    # Encode labels\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X, y_enc, df.index, test_size=0.3, random_state=42, stratify=y_enc)\n",
    "\n",
    "    # Train RandomForest\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_, output_dict=True)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Baseline: rule-based using only word_count\n",
    "    baseline_pred = []\n",
    "    for row in X_test:\n",
    "        wc = row[0]\n",
    "        if wc > 1500:\n",
    "            baseline_pred.append(le.transform(['High'])[0])\n",
    "        elif wc < 500:\n",
    "            baseline_pred.append(le.transform(['Low'])[0])\n",
    "        else:\n",
    "            baseline_pred.append(le.transform(['Medium'])[0])\n",
    "    baseline_acc = accuracy_score(y_test, baseline_pred)\n",
    "\n",
    "    # Feature importances\n",
    "    importances = clf.feature_importances_\n",
    "    feat_imp = sorted(list(zip(features_to_use, importances)), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # assemble results dataframe for test set\n",
    "    results_df = df.loc[idx_test].copy()\n",
    "    results_df['true_label'] = le.inverse_transform(y_test)\n",
    "    results_df['pred_label'] = le.inverse_transform(y_pred)\n",
    "\n",
    "    summary = {\n",
    "        'accuracy': acc,\n",
    "        'f1_weighted': f1,\n",
    "        'baseline_accuracy': baseline_acc,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'feature_importances': feat_imp\n",
    "    }\n",
    "\n",
    "    return clf, le, summary, results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2afe1-7310-4f61-8c7d-4855096680e1",
   "metadata": {},
   "source": [
    "## Real-time analysis function analyze_url:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b4c912-6477-4807-b19d-08406b3d9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_url(url, existing_features_df, embeddings, tfidf_model, svd_model, similarity_threshold=SIMILARITY_THRESHOLD):\n",
    "    html = safe_get(url)\n",
    "    if not html:\n",
    "        return {'url': url, 'error': 'Failed to fetch URL'}\n",
    "\n",
    "    extracted = extract_from_html(html, url=url)\n",
    "    text_clean = extracted['body_text'].lower().strip()\n",
    "    sentence_count = len(sent_tokenize(text_clean)) if text_clean else 0\n",
    "    flesch = compute_readability(text_clean) if text_clean else np.nan\n",
    "\n",
    "    # === NEW EMBEDDING LOGIC ===\n",
    "    # Use the FITTED models to transform the new text\n",
    "    try:\n",
    "        X_tfidf_new = tfidf_model.transform([text_clean])\n",
    "        emb_vector_reduced = svd_model.transform(X_tfidf_new)\n",
    "        \n",
    "        # Normalize\n",
    "        norm = np.linalg.norm(emb_vector_reduced)\n",
    "        if norm == 0: norm = 1.0\n",
    "        emb_vector_normalized = emb_vector_reduced / norm\n",
    "        \n",
    "        # This vector is shape (1, n_comps), e.g., (1, 81)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during embedding transform: {e}\")\n",
    "        # Create a zero vector of the correct shape so comparison doesn't fail\n",
    "        emb_vector_normalized = np.zeros((1, embeddings.shape[1])) \n",
    "                \n",
    "    # === COMPARE TO EXISTING EMBEDDINGS ===\n",
    "    similar_to = []\n",
    "    if existing_features_df is not None and embeddings is not None and len(embeddings) > 0:\n",
    "        # emb_vector_normalized is (1, n_comps). embeddings is (81, n_comps). This works.\n",
    "        sims = cosine_similarity(emb_vector_normalized, embeddings)[0]\n",
    "        for i, s in enumerate(sims):\n",
    "            if s >= similarity_threshold:\n",
    "                similar_to.append({'url': existing_features_df.iloc[i]['url'], 'similarity': float(s)})\n",
    "\n",
    "    # quality label rules\n",
    "    wc = extracted['word_count']\n",
    "    if wc > 1500 and 50 <= flesch <= 70:\n",
    "        label = 'High'\n",
    "    elif wc < 500 or (not np.isnan(flesch) and flesch < 30):\n",
    "        label = 'Low'\n",
    "    else:\n",
    "        label = 'Medium'\n",
    "\n",
    "    result = {\n",
    "        'url': url,\n",
    "        'word_count': wc,\n",
    "        'readability': flesch,\n",
    "        'quality_label': label,\n",
    "        'is_thin': wc < 500,\n",
    "        'similar_to': similar_to\n",
    "    }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc00cf77-4035-476b-b34c-a26f071f7d0a",
   "metadata": {},
   "source": [
    "## Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c17251-3c1e-4eef-9f24-d8086615ec95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 'build_extracted_csv' to parse all 81 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:   2%|█▋                                                                | 2/81 [00:00<00:07,  9.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips: 403 Client Error: Forbidden for url: https://www.qnbtrust.bank/Resources/Learning-Center/Blog/7-cyber-security-tips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  22%|██████████████▍                                                  | 18/81 [00:07<00:19,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.hpe.com/us/en/what-is/sd-wan.html: HTTPSConnectionPool(host='www.hpe.com', port=443): Read timed out. (read timeout=10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  28%|██████████████████▍                                              | 23/81 [00:22<01:24,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.cloudflare.com/learning/access-management/what-is-ztna/: 403 Client Error: Forbidden for url: https://www.cloudflare.com/learning/access-management/what-is-ztna/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  48%|███████████████████████████████▎                                 | 39/81 [00:26<00:05,  7.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d: 404 Client Error: Not Found for url: https://towardsdatascience.com/machine-learning-basics-with-examples-part-1-c2d37247ec3d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  51%|████████████████████████████████▉                                | 41/81 [00:28<00:21,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/: 404 Client Error: Not Found for url: https://www.analyticsvidhya.com/blog/2021/09/comprehensive-guide-on-machine-learning/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  62%|████████████████████████████████████████                         | 50/81 [00:32<00:08,  3.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.investopedia.com/terms/s/seo.asp: 404 Client Error: Not Found for url: https://www.investopedia.com/terms/s/seo.asp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  91%|███████████████████████████████████████████████████████████▍     | 74/81 [00:45<00:02,  2.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.reuters.com/technology/artificial-intelligence/: 401 Client Error: HTTP Forbidden for url: https://www.reuters.com/technology/artificial-intelligence/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  94%|████████████████████████████████████████████████████████████▉    | 76/81 [00:47<00:03,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.cnbc.com/artificial-intelligence/: 404 Client Error: Not Found for url: https://www.cnbc.com/artificial-intelligence/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows:  95%|█████████████████████████████████████████████████████████████▊   | 77/81 [00:52<00:06,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://www.bbc.com/news/topics/c404v061z99t: 404 Client Error: Not Found for url: https://www.bbc.com/news/topics/c404v061z99t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 100%|█████████████████████████████████████████████████████████████████| 81/81 [00:55<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted content to G:/seo-content-detector/data/extracted_pages.csv\n",
      "Extraction complete.\n",
      "Fitting TF-IDF and SVD models...\n",
      "Created embeddings with shape (81, 81)\n",
      "Saved features to G:/seo-content-detector/data/features.csv\n",
      "\n",
      "Running duplicate detection...\n",
      "Saved 4 duplicate pairs to G:/seo-content-detector/data/duplicates.csv\n",
      "\n",
      "Training classification model...\n",
      "\n",
      "--- Model Training Summary ---\n",
      "{\n",
      "  \"accuracy\": 0.92,\n",
      "  \"f1_weighted\": 0.914074074074074,\n",
      "  \"baseline_accuracy\": 0.36,\n",
      "  \"classification_report\": {\n",
      "    \"High\": {\n",
      "      \"precision\": 1.0,\n",
      "      \"recall\": 0.5,\n",
      "      \"f1-score\": 0.6666666666666666,\n",
      "      \"support\": 2.0\n",
      "    },\n",
      "    \"Low\": {\n",
      "      \"precision\": 0.9285714285714286,\n",
      "      \"recall\": 1.0,\n",
      "      \"f1-score\": 0.9629629629629629,\n",
      "      \"support\": 13.0\n",
      "    },\n",
      "    \"Medium\": {\n",
      "      \"precision\": 0.9,\n",
      "      \"recall\": 0.9,\n",
      "      \"f1-score\": 0.9,\n",
      "      \"support\": 10.0\n",
      "    },\n",
      "    \"accuracy\": 0.92,\n",
      "    \"macro avg\": {\n",
      "      \"precision\": 0.942857142857143,\n",
      "      \"recall\": 0.7999999999999999,\n",
      "      \"f1-score\": 0.8432098765432099,\n",
      "      \"support\": 25.0\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "      \"precision\": 0.9228571428571428,\n",
      "      \"recall\": 0.92,\n",
      "      \"f1-score\": 0.914074074074074,\n",
      "      \"support\": 25.0\n",
      "    }\n",
      "  },\n",
      "  \"confusion_matrix\": [\n",
      "    [\n",
      "      1,\n",
      "      0,\n",
      "      1\n",
      "    ],\n",
      "    [\n",
      "      0,\n",
      "      13,\n",
      "      0\n",
      "    ],\n",
      "    [\n",
      "      0,\n",
      "      1,\n",
      "      9\n",
      "    ]\n",
      "  ],\n",
      "  \"feature_importances\": [\n",
      "    [\n",
      "      \"flesch_reading_ease\",\n",
      "      0.40346830563434066\n",
      "    ],\n",
      "    [\n",
      "      \"word_count\",\n",
      "      0.3238750005896616\n",
      "    ],\n",
      "    [\n",
      "      \"sentence_count\",\n",
      "      0.27265669377599777\n",
      "    ]\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    if not os.path.exists(INPUT_CSV_PATH):\n",
    "        print(f\"Input CSV not found at {INPUT_CSV_PATH}.\")\n",
    "        print(\"Please make sure 'data/data.csv' exists.\")\n",
    "    else:\n",
    "        # --- ORIGINAL CODE ---\n",
    "        # 1) We are now RUNNING the slow function to create the file.\n",
    "        print(\"Running 'build_extracted_csv' to parse all 81 rows...\")\n",
    "        extracted_df = build_extracted_csv(INPUT_CSV_PATH, EXTRACTED_CSV)\n",
    "        print(\"Extraction complete.\")\n",
    "        # --- END OF ORIGINAL CODE ---\n",
    "\n",
    "        if extracted_df.empty or extracted_df['word_count'].sum() == 0:\n",
    "            print(\"Extraction resulted in no data. Stopping pipeline.\")\n",
    "        else:\n",
    "            # 2) Build features and get fitted models\n",
    "            # These variables are made global so the next cells can use them\n",
    "            global features_df, embeddings, global_tfidf_model, global_svd_model\n",
    "            features_df, embeddings, global_tfidf_model, global_svd_model = build_features(extracted_df, FEATURES_CSV)\n",
    "            embeddings = np.array(embeddings) \n",
    "            \n",
    "            # 3) Duplicate detection\n",
    "            print(\"\\nRunning duplicate detection...\")\n",
    "            duplicates_df = detect_duplicates(features_df, embeddings, SIMILARITY_THRESHOLD, DUPLICATES_CSV)\n",
    "\n",
    "            # 4) Create labels and train classifier\n",
    "            print(\"\\nTraining classification model...\")\n",
    "            features_labeled = create_quality_labels(features_df)\n",
    "            \n",
    "            # 'clf' is our trained classifier\n",
    "            global clf, le\n",
    "            clf, le, summary, results_df = train_quality_model(features_labeled)\n",
    "\n",
    "            if clf is not None:\n",
    "                # Save quality results\n",
    "                os.makedirs(os.path.dirname(QUALITY_CSV), exist_ok=True)\n",
    "                results_df.to_csv(QUALITY_CSV, index=False)\n",
    "                print(\"\\n--- Model Training Summary ---\")\n",
    "                print(json.dumps(summary, indent=2))\n",
    "            else:\n",
    "                print(\"Model training was skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4d95e-9a5a-49dc-bb1d-0b27a2fc7b1a",
   "metadata": {},
   "source": [
    "## Testing real-time analyzer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33608abb-7dac-48d9-9379-4862674cfa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing All 3 Real-Time Analyzer Cases ---\n",
      "\n",
      "\n",
      "==============================================\n",
      "Running Case: 1\n",
      "Analyzing URL: https://www.google.com/\n",
      "----------------------------------------------\n",
      "{\n",
      "  \"url\": \"https://www.google.com/\",\n",
      "  \"word_count\": 6,\n",
      "  \"readability\": 62.79000000000002,\n",
      "  \"quality_label\": \"Low\",\n",
      "  \"is_thin\": true,\n",
      "  \"similar_to\": []\n",
      "}\n",
      "==============================================\n",
      "\n",
      "\n",
      "==============================================\n",
      "Running Case: 2\n",
      "Analyzing URL: https://www.wikipedia.org/\n",
      "----------------------------------------------\n",
      "{\n",
      "  \"url\": \"https://www.wikipedia.org/\",\n",
      "  \"word_count\": 934,\n",
      "  \"readability\": 45.875150334075755,\n",
      "  \"quality_label\": \"Medium\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": []\n",
      "}\n",
      "==============================================\n",
      "\n",
      "\n",
      "==============================================\n",
      "Running Case: 3\n",
      "Analyzing URL: https://en.wikipedia.org/wiki/Machine_learning\n",
      "----------------------------------------------\n",
      "{\n",
      "  \"url\": \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
      "  \"word_count\": 18773,\n",
      "  \"readability\": 30.68587377490718,\n",
      "  \"quality_label\": \"Medium\",\n",
      "  \"is_thin\": false,\n",
      "  \"similar_to\": [\n",
      "    {\n",
      "      \"url\": \"https://en.wikipedia.org/wiki/Machine_learning\",\n",
      "      \"similarity\": 1.0000000000000002\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# --- FIX 1: This must be a dictionary (key: value pairs) ---\n",
    "test_cases = {\n",
    "    \"1\": \"https://www.google.com/\",\n",
    "    \"2\": \"https://www.wikipedia.org/\",\n",
    "    \"3\": \"https://en.wikipedia.org/wiki/Machine_learning\"\n",
    "}\n",
    "\n",
    "print(\"--- Testing All 3 Real-Time Analyzer Cases ---\")\n",
    "\n",
    "try:\n",
    "    # This loop now works because test_cases is a dict\n",
    "    for case_name, test_url in test_cases.items():\n",
    "        print(f\"\\n\\n==============================================\")\n",
    "        print(f\"Running Case: {case_name}\")\n",
    "        print(f\"Analyzing URL: {test_url}\")\n",
    "        print(\"----------------------------------------------\")\n",
    "        \n",
    "        # --- FIX 2: Cleaned up the function call (removed stray characters and spaces) ---\n",
    "        result = analyze_url(test_url, \n",
    "                             existing_features_df=features_df, \n",
    "                             embeddings=embeddings, \n",
    "                             tfidf_model=global_tfidf_model,  # Pass the model\n",
    "                             svd_model=global_svd_model,      # Pass the model\n",
    "                             similarity_threshold=SIMILARITY_THRESHOLD)\n",
    "        \n",
    "        print(json.dumps(result, indent=2))\n",
    "        print(f\"==============================================\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\n\\n[ERROR] A required variable was not found. ({e})\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n\\n[ERROR] An error occurred while analyzing: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f8b649b-418d-4668-996c-9cad5b38bc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving all files for Streamlit app ---\n",
      "Successfully saved model to ../models/quality_model.pkl\n",
      "Successfully saved TF-IDF model to ../models/tfidf_model.pkl\n",
      "Successfully saved SVD model to ../models/svd_model.pkl\n",
      "Successfully saved embeddings to ../data/embeddings.npy\n",
      "\n",
      "All files saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Define all file paths ---\n",
    "MODEL_PATH = \"../models/quality_model.pkl\"\n",
    "TFIDF_PATH = \"../models/tfidf_model.pkl\"\n",
    "SVD_PATH = \"../models/svd_model.pkl\"\n",
    "EMBEDDINGS_PATH = \"../data/embeddings.npy\"\n",
    "\n",
    "print(\"\\n--- Saving all files for Streamlit app ---\")\n",
    "\n",
    "try:\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(EMBEDDINGS_PATH), exist_ok=True)\n",
    "\n",
    "    # 1. Save the classification model (clf)\n",
    "    joblib.dump(clf, MODEL_PATH)\n",
    "    print(f\"Successfully saved model to {MODEL_PATH}\")\n",
    "    \n",
    "    # 2. Save the TF-IDF model\n",
    "    joblib.dump(global_tfidf_model, TFIDF_PATH)\n",
    "    print(f\"Successfully saved TF-IDF model to {TFIDF_PATH}\")\n",
    "    \n",
    "    # 3. Save the SVD model\n",
    "    joblib.dump(global_svd_model, SVD_PATH)\n",
    "    print(f\"Successfully saved SVD model to {SVD_PATH}\")\n",
    "    \n",
    "    # 4. Save the embeddings array\n",
    "    np.save(EMBEDDINGS_PATH, embeddings)\n",
    "    print(f\"Successfully saved embeddings to {EMBEDDINGS_PATH}\")\n",
    "    \n",
    "    print(\"\\nAll files saved successfully!\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"\\n[ERROR] A required variable was not found. ({e})\")\n",
    "    print(\"Please make sure you have run the main pipeline cell (Cell 5) first.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] An error occurred while saving the files: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
